{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load counts from CSV\n",
    "def load_csv_counts(filename):\n",
    "    counts = {}\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        header = f.readline()\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\",\")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            ngram_str = parts[0].strip().strip('\"')\n",
    "            count_str = parts[1].strip().strip('\"')\n",
    "            try:\n",
    "                count = int(count_str)\n",
    "            except:\n",
    "                continue\n",
    "            ngram = tuple(ngram_str.split())\n",
    "            if len(ngram) == 0:\n",
    "                continue\n",
    "            counts[ngram] = count\n",
    "    return counts\n",
    "\n",
    "# Load trigram and quadrigram\n",
    "tri_counts  = load_csv_counts(r\"C:\\Users\\dubey\\OneDrive\\Desktop\\Coding\\NLP-lab\\LAB4\\trigram.csv\")\n",
    "quad_counts = load_csv_counts(r\"C:\\Users\\dubey\\OneDrive\\Desktop\\Coding\\NLP-lab\\LAB4\\quadrigram.csv\")\n",
    "\n",
    "ngram_counts = {3: tri_counts, 4: quad_counts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9808043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_prob(word, context, counts_dicts, n):\n",
    "    \"\"\"\n",
    "    Returns MLE probability of 'word' given 'context'.\n",
    "    Uses backoff if context not found.\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        total = sum(counts_dicts[3].values())  # fallback: use trigram counts last word frequencies\n",
    "        word_counts = sum(c for g, c in counts_dicts[3].items() if g[-1]==word)\n",
    "        return word_counts / total if total > 0 else 0\n",
    "\n",
    "    ctx = tuple(context[-(n-1):])\n",
    "    ngram = ctx + (word,)\n",
    "    c_ngram = counts_dicts[n].get(ngram, 0)\n",
    "    c_prefix = sum([c for g, c in counts_dicts[n].items() if g[:-1] == ctx])\n",
    "    if c_prefix == 0:\n",
    "        return get_prob(word, context, counts_dicts, n-1)\n",
    "    return c_ngram / c_prefix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478fd496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_context(counts_dicts, n):\n",
    "    # Choose a context that exists in the n-grams and starts with <s>\n",
    "    candidates = [g[:-1] for g in counts_dicts[n] if g[0] == \"<s>\"]\n",
    "    if candidates:\n",
    "        return list(random.choice(candidates))\n",
    "    else:\n",
    "        return [\"<s>\"] * (n-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c2b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_greedy_ng(counts_dicts, n, max_len=15):\n",
    "    sentence = get_start_context(counts_dicts, n)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        ctx = tuple(sentence[-(n-1):])\n",
    "        candidates_dict = {g[-1]: counts_dicts[n][g] for g in counts_dicts[n] if g[:-1]==ctx}\n",
    "        \n",
    "        if not candidates_dict:\n",
    "            # fallback: pick any last word from trigram\n",
    "            last_words = [g[-1] for g in counts_dicts[3]]\n",
    "            next_word = random.choice(last_words)\n",
    "            sentence.append(next_word)\n",
    "            if next_word == \"</s>\":\n",
    "                break\n",
    "            continue\n",
    "        \n",
    "        # Probabilistic sampling\n",
    "        words, counts = zip(*candidates_dict.items())\n",
    "        probs = [c/sum(counts) for c in counts]\n",
    "        next_word = random.choices(words, probs)[0]\n",
    "        sentence.append(next_word)\n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "    return \" \".join(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20028cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_beam_ng(counts_dicts, n, beam_size=20, max_len=15):\n",
    "    sequences = [(get_start_context(counts_dicts, n), 1.0)]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "        for seq, seq_prob in sequences:\n",
    "            ctx = tuple(seq[-(n-1):])\n",
    "            candidates_dict = {g[-1]: counts_dicts[n][g] for g in counts_dicts[n] if g[:-1]==ctx}\n",
    "            \n",
    "            if not candidates_dict:\n",
    "                # fallback: pick any last word from trigram\n",
    "                last_words = [g[-1] for g in counts_dicts[3]]\n",
    "                next_word = random.choice(last_words)\n",
    "                all_candidates.append((seq+[next_word], seq_prob))\n",
    "                continue\n",
    "            \n",
    "            words, counts = zip(*candidates_dict.items())\n",
    "            probs = [c/sum(counts) for c in counts]\n",
    "            for w, p in zip(words, probs):\n",
    "                all_candidates.append((seq+[w], seq_prob * p))\n",
    "        \n",
    "        all_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        sequences = all_candidates[:beam_size]\n",
    "        \n",
    "        if all(seq[-1] == \"</s>\" for seq, _ in sequences):\n",
    "            break\n",
    "    return [\" \".join(seq) for seq, _ in sequences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f3419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 3-gram greedy & beam sentences saved.\n",
      "✅ 4-gram greedy & beam sentences saved.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "n_values = [3,4]  # trigram and quadrigram\n",
    "num_sentences = 100\n",
    "\n",
    "for n in n_values:\n",
    "    # Greedy\n",
    "    greedy_sentences = [generate_greedy_ng(ngram_counts, n) for _ in range(num_sentences)]\n",
    "    output_file = f\"C:\\\\Users\\\\Dubey\\\\OneDrive\\\\Desktop\\\\Coding\\\\NLP-Lab\\\\LAB6\\\\greedy_{n}gram_100.csv\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Sentence\"])\n",
    "        for s in greedy_sentences:\n",
    "            writer.writerow([s])\n",
    "    \n",
    "    # Beam search\n",
    "    beam_sentences = []\n",
    "    while len(beam_sentences) < num_sentences:\n",
    "        seqs = generate_beam_ng(ngram_counts, n, beam_size=20)\n",
    "        beam_sentences.extend(seqs)\n",
    "    beam_sentences = beam_sentences[:num_sentences]\n",
    "    \n",
    "    output_file_beam = f\"C:\\\\Users\\\\Dubey\\\\OneDrive\\\\Desktop\\\\Coding\\\\NLP-Lab\\\\LAB6\\\\beam_{n}gram_100.csv\"\n",
    "    with open(output_file_beam, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Sentence\"])\n",
    "        for s in beam_sentences:\n",
    "            writer.writerow([s])\n",
    "    \n",
    "    print(f\"✅ {n}-gram greedy & beam sentences saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
