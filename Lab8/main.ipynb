{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445db483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial State ---\n",
      "Initial Vocabulary Size: 21\n",
      "--------------------\n",
      "Iteration 1: Merged ('<', '/') -> '</' (Frequency: 31)\n",
      "Iteration 2: Merged ('</', 'w') -> '</w' (Frequency: 31)\n",
      "Iteration 3: Merged ('</w', '>') -> '</w>' (Frequency: 31)\n",
      "Iteration 4: Merged ('e', '</w>') -> 'e</w>' (Frequency: 12)\n",
      "Iteration 5: Merged ('t', 'h') -> 'th' (Frequency: 10)\n",
      "Iteration 6: Merged ('th', 'e</w>') -> 'the</w>' (Frequency: 10)\n",
      "Iteration 7: Merged ('s', '</w>') -> 's</w>' (Frequency: 6)\n",
      "Iteration 8: Merged ('g', '</w>') -> 'g</w>' (Frequency: 5)\n",
      "Iteration 9: Merged ('d', 'o') -> 'do' (Frequency: 4)\n",
      "Iteration 10: Merged ('b', 'o') -> 'bo' (Frequency: 3)\n",
      "Iteration 11: Merged ('bo', 'y') -> 'boy' (Frequency: 3)\n",
      "Iteration 12: Merged ('g', 's</w>') -> 'gs</w>' (Frequency: 3)\n",
      "Iteration 13: Merged ('c', 'a') -> 'ca' (Frequency: 3)\n",
      "Iteration 14: Merged ('ca', 't') -> 'cat' (Frequency: 3)\n",
      "Iteration 15: Merged ('i', 'n') -> 'in' (Frequency: 3)\n",
      "Iteration 16: Merged ('in', 'g</w>') -> 'ing</w>' (Frequency: 3)\n",
      "Iteration 17: Merged ('boy', '</w>') -> 'boy</w>' (Frequency: 2)\n",
      "Iteration 18: Merged ('h', 'u') -> 'hu' (Frequency: 2)\n",
      "Iteration 19: Merged ('cat', '</w>') -> 'cat</w>' (Frequency: 2)\n",
      "Iteration 20: Merged ('a', 'r') -> 'ar' (Frequency: 2)\n",
      "\n",
      "--- Final Vocabulary ---\n",
      "Final Vocabulary Size: 41\n",
      "['/', '<', '</', '</w', '</w>', '>', 'a', 'ar', 'b', 'bo', 'boy', 'boy</w>', 'c', 'ca', 'cat', 'cat</w>', 'd', 'do', 'e', 'e</w>', 'g', 'g</w>', 'gs</w>', 'h', 'hu', 'i', 'in', 'ing</w>', 'l', 'n', 'o', 'q', 'r', 's', 's</w>', 't', 'th', 'the</w>', 'u', 'w', 'y']\n",
      "--------------------\n",
      "\n",
      "--- Tokenizing New Sentence ---\n",
      "'The cat is chasing the dog quietly.'\n",
      "\n",
      "'the' -> ['the</w>']\n",
      "'cat' -> ['cat</w>']\n",
      "'is' -> ['i', 's</w>']\n",
      "'chasing' -> ['c', 'h', 'a', 's', 'ing</w>']\n",
      "'the' -> ['the</w>']\n",
      "'dog' -> ['do', 'g</w>']\n",
      "'quietly' -> ['q', 'u', 'i', 'e', 't', 'l', 'y', '</w>']\n",
      "\n",
      "Final Tokenized Output: [['the</w>'], ['cat</w>'], ['i', 's</w>'], ['c', 'h', 'a', 's', 'ing</w>'], ['the</w>'], ['do', 'g</w>'], ['q', 'u', 'i', 'e', 't', 'l', 'y', '</w>']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "def get_word_frequencies(sentences):\n",
    "    \"\"\"\n",
    "    Preprocesses sentences and returns a frequency dictionary of words.\n",
    "    Lowercase, remove periods, and add </w> to each word.\n",
    "    \"\"\"\n",
    "    word_freq = collections.defaultdict(int)\n",
    "    for sentence in sentences:\n",
    "        # Lowercase, remove period, and split into words\n",
    "        words = sentence.lower().replace('.', '').split()\n",
    "        for word in words:\n",
    "            # Append end-of-word token and count frequency\n",
    "            word_freq[word + '</w>'] += 1\n",
    "    return word_freq\n",
    "\n",
    "def initialize_corpus(word_freq):\n",
    "    \"\"\"\n",
    "    Splits each word into characters, creating the initial corpus representation.\n",
    "    Example: {'the</w>': 8} -> {'t h e </w>': 8}\n",
    "    \"\"\"\n",
    "    corpus = {}\n",
    "    for word, freq in word_freq.items():\n",
    "        corpus[\" \".join(list(word))] = freq\n",
    "    return corpus\n",
    "\n",
    "def initialize_vocab(corpus):\n",
    "    \"\"\"Initializes the vocabulary with all unique characters.\"\"\"\n",
    "    vocab = set()\n",
    "    for word in corpus:\n",
    "        vocab.update(word.split())\n",
    "    return list(vocab)\n",
    "\n",
    "def get_pair_stats(corpus):\n",
    "    \"\"\"Counts the frequency of each adjacent pair of symbols.\"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in corpus.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # Add the frequency of the word to the pair's count\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_pair(best_pair, corpus):\n",
    "    \"\"\"Merges the most frequent pair in the corpus.\"\"\"\n",
    "    new_corpus = {}\n",
    "    # Create the merged token (e.g., ('e', '</w>') -> 'e</w>')\n",
    "    new_token = \"\".join(best_pair)\n",
    "    # Use regex to replace the pair in the keys of the corpus dictionary\n",
    "    # We escape special characters and handle the space between tokens\n",
    "    pattern = re.compile(r'(?<!\\S)' + re.escape(' '.join(best_pair)) + r'(?!\\S)')\n",
    "    \n",
    "    for word, freq in corpus.items():\n",
    "        new_word = pattern.sub(new_token, word)\n",
    "        new_corpus[new_word] = freq\n",
    "        \n",
    "    return new_corpus\n",
    "\n",
    "def tokenize_word(input_word, vocab):\n",
    "    \"\"\"\n",
    "    Tokenizes a word using the greedy longest-match algorithm.\n",
    "    \"\"\"\n",
    "    # Ensure the word has the end-of-word token\n",
    "    if not input_word.endswith('</w>'):\n",
    "        input_word += '</w>'\n",
    "\n",
    "    tokens = []\n",
    "    current_pos = 0\n",
    "    while current_pos < len(input_word):\n",
    "        # Find the longest subword in the vocab that is a prefix\n",
    "        best_match = \"\"\n",
    "        for i in range(current_pos, len(input_word)):\n",
    "            subword = input_word[current_pos : i+1]\n",
    "            if subword in vocab:\n",
    "                best_match = subword\n",
    "        \n",
    "        # If no match is found (should only happen for unknown chars),\n",
    "        # treat the single character as the token.\n",
    "        if not best_match:\n",
    "            best_match = input_word[current_pos]\n",
    "\n",
    "        tokens.append(best_match)\n",
    "        current_pos += len(best_match)\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Dataset Initialization\n",
    "    dataset = [\n",
    "        \"The boy hugs the cat.\",\n",
    "        \"The boys are hugging the dogs.\",\n",
    "        \"The dogs are chasing the cats.\",\n",
    "        \"The dog and the cat sit quietly.\",\n",
    "        \"The boy is sitting on the dog.\"\n",
    "    ]\n",
    "    num_merges = 20\n",
    "\n",
    "    # 2. Preprocessing\n",
    "    word_frequencies = get_word_frequencies(dataset)\n",
    "    corpus = initialize_corpus(word_frequencies)\n",
    "    vocab = initialize_vocab(corpus)\n",
    "\n",
    "    print(\"--- Initial State ---\")\n",
    "    print(f\"Initial Vocabulary Size: {len(vocab)}\")\n",
    "    # print(f\"Initial Vocabulary: {sorted(vocab)}\")\n",
    "    # print(f\"Initial Corpus: {corpus}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # 3. Training Loop (Applying Merges)\n",
    "    for i in range(num_merges):\n",
    "        pair_stats = get_pair_stats(corpus)\n",
    "        if not pair_stats:\n",
    "            break # No more pairs to merge\n",
    "        \n",
    "        # Find the most frequent pair\n",
    "        # The key for max is a lambda function to look up the frequency in pair_stats\n",
    "        # Tie-breaking is handled implicitly by Python's default behavior for max()\n",
    "        best_pair = max(pair_stats, key=pair_stats.get)\n",
    "        \n",
    "        # Perform the merge\n",
    "        corpus = merge_pair(best_pair, corpus)\n",
    "        \n",
    "        # Add the new merged token to the vocabulary\n",
    "        new_token = \"\".join(best_pair)\n",
    "        vocab.append(new_token)\n",
    "        \n",
    "        print(f\"Iteration {i+1}: Merged {best_pair} -> '{new_token}' (Frequency: {pair_stats[best_pair]})\")\n",
    "\n",
    "    # 4. Final Vocabulary\n",
    "    print(\"\\n--- Final Vocabulary ---\")\n",
    "    print(f\"Final Vocabulary Size: {len(vocab)}\")\n",
    "    print(sorted(vocab))\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # 5. Tokenize a new sentence\n",
    "    new_sentence = \"The cat is chasing the dog quietly.\"\n",
    "    print(f\"\\n--- Tokenizing New Sentence ---\\n'{new_sentence}'\\n\")\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    for word in new_sentence.lower().replace('.', '').split():\n",
    "        tokens = tokenize_word(word, vocab)\n",
    "        tokenized_sentence.append(tokens)\n",
    "        print(f\"'{word}' -> {tokens}\")\n",
    "    \n",
    "    print(\"\\nFinal Tokenized Output:\", tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e96f44bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Preprocessing Training Data ---\n",
      "Original: 'Check out https://example.com for more info!'\n",
      "Processed: ['check', 'out', 'url', 'for', 'more', 'info', 'punct']\n",
      "\n",
      "Original: 'Order 3 items, get 1 free! Limited offer!!!'\n",
      "Processed: ['order', 'number', 'items', 'punct', 'get', 'number', 'free', 'punct', 'limited', 'offer', 'punct', 'punct', 'punct']\n",
      "\n",
      "Original: 'Your package #12345 will arrive tomorrow.'\n",
      "Processed: ['your', 'package', 'number', 'will', 'arrive', 'tomorrow', 'punct']\n",
      "\n",
      "Original: 'Win $1000 now, visit http://winbig.com!!!'\n",
      "Processed: ['win', 'number', 'now', 'punct', 'visit', 'url']\n",
      "\n",
      "Original: 'Meeting at 3pm, don't forget to bring the files.'\n",
      "Processed: ['meeting', 'at', 'numberpm', 'punct', \"don't\", 'forget', 'to', 'bring', 'the', 'files', 'punct']\n",
      "\n",
      "Original: 'Exclusive deal for you: buy 2, get 1 free!!!'\n",
      "Processed: ['exclusive', 'deal', 'for', 'you', 'punct', 'buy', 'number', 'punct', 'get', 'number', 'free', 'punct', 'punct', 'punct']\n",
      "\n",
      "Original: 'Download the report from https://reports.com.'\n",
      "Processed: ['download', 'the', 'report', 'from', 'url']\n",
      "\n",
      "Original: 'The meeting is starting in 10 minutes.'\n",
      "Processed: ['the', 'meeting', 'is', 'starting', 'in', 'number', 'minutes', 'punct']\n",
      "\n",
      "Original: 'Reminder: submit your timesheet by 5pm today.'\n",
      "Processed: ['reminder', 'punct', 'submit', 'your', 'timesheet', 'by', 'numberpm', 'today', 'punct']\n",
      "\n",
      "\n",
      "--- 2. Trained Model Parameters ---\n",
      "\n",
      "[A] Class Priors P(Class):\n",
      "Inform    : 0.3333\n",
      "Promo     : 0.3333\n",
      "Reminder  : 0.3333\n",
      "\n",
      "[B] Specific Feature Probabilities P(Feature=1 | Class):\n",
      "Feature        Inform    Promo     Reminder  \n",
      "---------------------------------------------\n",
      "has_URL        0.0000    0.0000    0.0000    \n",
      "has_NUMBER     0.0000    0.0000    0.0000    \n",
      "has_PUNCT      0.0000    0.0000    0.0000    \n",
      "\n",
      "[C] Bigram Smoothing Parameters:\n",
      "Total Unique Bigrams |V_bigram|: 64\n",
      "Total Bigrams per Class (N_class):\n",
      "Inform    : 16\n",
      "Promo     : 30\n",
      "Reminder  : 25\n",
      "\n",
      "[D] Example Bigram Probabilities (for relevant bigrams):\n",
      "Bigram                   P(bigram|Info)P(bigram|Prom)P(bigram|Remi)\n",
      "-------------------------------------------------------------------\n",
      "('reminder', 'punct')    0.0085            0.0061            0.0294            \n",
      "('punct', 'submit')      0.0085            0.0061            0.0294            \n",
      "('submit', 'your')       0.0085            0.0061            0.0294            \n",
      "('your', 'timesheet')    0.0085            0.0061            0.0294            \n",
      "('timesheet', 'by')      0.0085            0.0061            0.0294            \n",
      "('by', 'numberpm')       0.0085            0.0061            0.0294            \n",
      "('numberpm', 'today')    0.0085            0.0061            0.0294            \n",
      "('today', 'punct')       0.0085            0.0061            0.0294            \n",
      "\n",
      "--- 3. Prediction Phase ---\n",
      "Test Sentence: 'You will get an exclusive offer in the meeting!'\n",
      "Processed Tokens: ['you', 'will', 'get', 'an', 'exclusive', 'offer', 'in', 'the', 'meeting', 'punct']\n",
      "\n",
      "--- Calculating Score for Class: Inform ---\n",
      "  Log Prior P(Inform): -1.0986\n",
      "  + Log P(has_URL=0 | Inform): 0.0000\n",
      "  + Log P(has_NUMBER=0 | Inform): 0.0000\n",
      "  + Log P(has_PUNCT=0 | Inform): 0.0000\n",
      "  + Sum of Log Bigram Probs: -4.7650\n",
      "  -------------------------------------\n",
      "  Total Log Score for Inform: -5.8636\n",
      "\n",
      "--- Calculating Score for Class: Promo ---\n",
      "  Log Prior P(Promo): -1.0986\n",
      "  + Log P(has_URL=0 | Promo): 0.0000\n",
      "  + Log P(has_NUMBER=0 | Promo): 0.0000\n",
      "  + Log P(has_PUNCT=0 | Promo): 0.0000\n",
      "  + Sum of Log Bigram Probs: -5.0999\n",
      "  -------------------------------------\n",
      "  Total Log Score for Promo: -6.1985\n",
      "\n",
      "--- Calculating Score for Class: Reminder ---\n",
      "  Log Prior P(Reminder): -1.0986\n",
      "  + Log P(has_URL=0 | Reminder): 0.0000\n",
      "  + Log P(has_NUMBER=0 | Reminder): 0.0000\n",
      "  + Log P(has_PUNCT=0 | Reminder): 0.0000\n",
      "  + Sum of Log Bigram Probs: -3.5264\n",
      "  -------------------------------------\n",
      "  Total Log Score for Reminder: -4.6250\n",
      "\n",
      "\n",
      "--- 4. Final Result ---\n",
      "Sentence to classify: 'You will get an exclusive offer in the meeting!'\n",
      "\n",
      "--- Final Log Scores ---\n",
      "Inform    : -5.8636\n",
      "Promo     : -6.1985\n",
      "Reminder  : -4.6250\n",
      "\n",
      "--------------------\n",
      "Predicted Label: Reminder\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"Applies all preprocessing rules to a single sentence.\"\"\"\n",
    "    s = re.sub(r'https?://\\S+', 'URL', sentence)\n",
    "    s = re.sub(r'[\\$#]?\\d+', 'NUMBER', s)\n",
    "    s = re.sub(r'[!,.?:]', ' PUNCT ', s)\n",
    "    tokens = s.lower().split()\n",
    "    return tokens\n",
    "\n",
    "def train_naive_bayes(data, k=0.3, verbose=False):\n",
    "    \"\"\"\n",
    "    Trains the Naive Bayes classifier and returns a model with all probabilities.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"--- 1. Preprocessing Training Data ---\")\n",
    "        for sentence, label in data:\n",
    "            print(f\"Original: '{sentence}'\")\n",
    "            print(f\"Processed: {preprocess_sentence(sentence)}\\n\")\n",
    "\n",
    "    class_doc_counts = collections.defaultdict(int)\n",
    "    feature_counts = { 'Inform': collections.defaultdict(int), 'Promo': collections.defaultdict(int), 'Reminder': collections.defaultdict(int) }\n",
    "    bigram_counts = { 'Inform': collections.defaultdict(int), 'Promo': collections.defaultdict(int), 'Reminder': collections.defaultdict(int) }\n",
    "    N_class = collections.defaultdict(int)\n",
    "    vocab_bigram = set()\n",
    "\n",
    "    for sentence, label in data:\n",
    "        class_doc_counts[label] += 1\n",
    "        tokens = preprocess_sentence(sentence)\n",
    "        if 'URL' in tokens: feature_counts[label]['has_URL'] += 1\n",
    "        if 'NUMBER' in tokens: feature_counts[label]['has_NUMBER'] += 1\n",
    "        if 'PUNCT' in tokens: feature_counts[label]['has_PUNCT'] += 1\n",
    "        bigrams = list(zip(tokens, tokens[1:]))\n",
    "        for bigram in bigrams:\n",
    "            bigram_counts[label][bigram] += 1\n",
    "            vocab_bigram.add(bigram)\n",
    "        N_class[label] += len(bigrams)\n",
    "            \n",
    "    total_docs = sum(class_doc_counts.values())\n",
    "    labels = list(class_doc_counts.keys())\n",
    "    \n",
    "    priors = {label: count / total_docs for label, count in class_doc_counts.items()}\n",
    "    \n",
    "    feature_probs = {l: {} for l in labels}\n",
    "    for label in labels:\n",
    "        for feature in ['has_URL', 'has_NUMBER', 'has_PUNCT']:\n",
    "            feature_probs[label][feature] = feature_counts[label][feature] / class_doc_counts[label]\n",
    "\n",
    "    V_size = len(vocab_bigram)\n",
    "    bigram_probs = {l: {} for l in labels}\n",
    "    for label in labels:\n",
    "        denominator = N_class[label] + k * V_size\n",
    "        for bigram in vocab_bigram:\n",
    "            numerator = bigram_counts[label].get(bigram, 0) + k\n",
    "            bigram_probs[label][bigram] = numerator / denominator\n",
    "\n",
    "    model = { 'priors': priors, 'feature_probs': feature_probs, 'bigram_probs': bigram_probs, 'labels': labels, 'bigram_vocab_size': V_size, 'N_class': N_class }\n",
    "    \n",
    "    if verbose:\n",
    "        print_model_details(model, data[-1][0]) # Using last sentence's bigrams as example\n",
    "        \n",
    "    return model\n",
    "\n",
    "def print_model_details(model, example_sentence):\n",
    "    \"\"\"Prints the key parameters of the trained model.\"\"\"\n",
    "    print(\"\\n--- 2. Trained Model Parameters ---\")\n",
    "    print(\"\\n[A] Class Priors P(Class):\")\n",
    "    for label, prior in model['priors'].items():\n",
    "        print(f\"{label:<10}: {prior:.4f}\")\n",
    "\n",
    "    print(\"\\n[B] Specific Feature Probabilities P(Feature=1 | Class):\")\n",
    "    header = f\"{'Feature':<15}\" + \"\".join([f\"{label:<10}\" for label in model['labels']])\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    for feature in ['has_URL', 'has_NUMBER', 'has_PUNCT']:\n",
    "        row = f\"{feature:<15}\"\n",
    "        for label in model['labels']:\n",
    "            row += f\"{model['feature_probs'][label][feature]:<10.4f}\"\n",
    "        print(row)\n",
    "\n",
    "    print(\"\\n[C] Bigram Smoothing Parameters:\")\n",
    "    print(f\"Total Unique Bigrams |V_bigram|: {model['bigram_vocab_size']}\")\n",
    "    print(\"Total Bigrams per Class (N_class):\")\n",
    "    for label, count in model['N_class'].items():\n",
    "        print(f\"{label:<10}: {count}\")\n",
    "\n",
    "    print(\"\\n[D] Example Bigram Probabilities (for relevant bigrams):\")\n",
    "    test_bigrams_list = list(zip(preprocess_sentence(example_sentence), preprocess_sentence(example_sentence)[1:]))\n",
    "    header = f\"{'Bigram':<25}\" + \"\".join([f\"P(bigram|{label[:4]})\" for label in model['labels']])\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    for bigram in test_bigrams_list:\n",
    "        row = f\"{str(bigram):<25}\"\n",
    "        for label in model['labels']:\n",
    "            prob = model['bigram_probs'][label].get(bigram, \"N/A\")\n",
    "            if isinstance(prob, float):\n",
    "                row += f\"{prob:<18.4f}\"\n",
    "            else:\n",
    "                row += f\"{'N/A':<18}\"\n",
    "        print(row)\n",
    "\n",
    "\n",
    "def predict(sentence, model, verbose=False):\n",
    "    \"\"\"Predicts the label for a new sentence with verbose output.\"\"\"\n",
    "    labels = model['labels']\n",
    "    tokens = preprocess_sentence(sentence)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n--- 3. Prediction Phase ---\")\n",
    "        print(f\"Test Sentence: '{sentence}'\")\n",
    "        print(f\"Processed Tokens: {tokens}\\n\")\n",
    "\n",
    "    has_URL = 1 if 'URL' in tokens else 0\n",
    "    has_NUMBER = 1 if 'NUMBER' in tokens else 0\n",
    "    has_PUNCT = 1 if 'PUNCT' in tokens else 0\n",
    "    test_bigrams = list(zip(tokens, tokens[1:]))\n",
    "    \n",
    "    scores = {}\n",
    "    for label in labels:\n",
    "        if verbose: print(f\"--- Calculating Score for Class: {label} ---\")\n",
    "        \n",
    "        log_score = np.log(model['priors'][label])\n",
    "        if verbose: print(f\"  Log Prior P({label}): {log_score:.4f}\")\n",
    "        \n",
    "        epsilon = 1e-9\n",
    "        \n",
    "        prob_url = model['feature_probs'][label]['has_URL']\n",
    "        log_prob = np.log(prob_url + epsilon) if has_URL else np.log(1 - prob_url + epsilon)\n",
    "        log_score += log_prob\n",
    "        if verbose: print(f\"  + Log P(has_URL={has_URL} | {label}): {log_prob:.4f}\")\n",
    "        \n",
    "        prob_num = model['feature_probs'][label]['has_NUMBER']\n",
    "        log_prob = np.log(prob_num + epsilon) if has_NUMBER else np.log(1 - prob_num + epsilon)\n",
    "        log_score += log_prob\n",
    "        if verbose: print(f\"  + Log P(has_NUMBER={has_NUMBER} | {label}): {log_prob:.4f}\")\n",
    "\n",
    "        prob_punct = model['feature_probs'][label]['has_PUNCT']\n",
    "        log_prob = np.log(prob_punct + epsilon) if has_PUNCT else np.log(1 - prob_punct + epsilon)\n",
    "        log_score += log_prob\n",
    "        if verbose: print(f\"  + Log P(has_PUNCT={has_PUNCT} | {label}): {log_prob:.4f}\")\n",
    "        \n",
    "        sum_log_bigram_probs = 0\n",
    "        for bigram in test_bigrams:\n",
    "            if bigram in model['bigram_probs'][label]:\n",
    "                sum_log_bigram_probs += np.log(model['bigram_probs'][label][bigram])\n",
    "        \n",
    "        log_score += sum_log_bigram_probs\n",
    "        if verbose: \n",
    "            print(f\"  + Sum of Log Bigram Probs: {sum_log_bigram_probs:.4f}\")\n",
    "            print(f\"  -------------------------------------\")\n",
    "            print(f\"  Total Log Score for {label}: {log_score:.4f}\\n\")\n",
    "\n",
    "        scores[label] = log_score\n",
    "        \n",
    "    return max(scores, key=scores.get), scores\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = [\n",
    "        (\"Check out https://example.com for more info!\", \"Inform\"),\n",
    "        (\"Order 3 items, get 1 free! Limited offer!!!\", \"Promo\"),\n",
    "        (\"Your package #12345 will arrive tomorrow.\", \"Inform\"),\n",
    "        (\"Win $1000 now, visit http://winbig.com!!!\", \"Promo\"),\n",
    "        (\"Meeting at 3pm, don't forget to bring the files.\", \"Reminder\"),\n",
    "        (\"Exclusive deal for you: buy 2, get 1 free!!!\", \"Promo\"),\n",
    "        (\"Download the report from https://reports.com.\", \"Inform\"),\n",
    "        (\"The meeting is starting in 10 minutes.\", \"Reminder\"),\n",
    "        (\"Reminder: submit your timesheet by 5pm today.\", \"Reminder\")\n",
    "    ]\n",
    "    \n",
    "    test_sentence = \"You will get an exclusive offer in the meeting!\"\n",
    "    \n",
    "    # Train the model with verbose=True to print training details\n",
    "    nb_model = train_naive_bayes(training_data, k=0.3, verbose=True)\n",
    "    \n",
    "    # Make prediction with verbose=True to print prediction steps\n",
    "    prediction, scores = predict(test_sentence, nb_model, verbose=True)\n",
    "    \n",
    "    print(\"\\n--- 4. Final Result ---\")\n",
    "    print(f\"Sentence to classify: '{test_sentence}'\\n\")\n",
    "    print(\"--- Final Log Scores ---\")\n",
    "    for label, score in scores.items():\n",
    "        print(f\"{label:<10}: {score:.4f}\")\n",
    "    \n",
    "    print(\"\\n--------------------\")\n",
    "    print(f\"Predicted Label: {prediction}\")\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536f16c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
