{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1a8595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- LOAD CORPUS -------------------------\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def load_corpus(path=\"corpus.txt\", lowercase=True):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "    except:\n",
    "        text = \"\"\"\n",
    "        this is a demo corpus used only if corpus.txt not found\n",
    "        byte pair encoding and wordpiece tokenization implemented\n",
    "        from scratch using python without any libraries\n",
    "        \"\"\"\n",
    "\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    words = [w for w in text.split(\" \") if w.strip() != \"\"]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612cd82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- BPE IMPLEMENTATION -------------------------\n",
    "def split_word(word):\n",
    "    return list(word) + [\"</w>\"]\n",
    "\n",
    "def build_vocab(words):\n",
    "    freq = Counter(words)\n",
    "    vocab = Counter()\n",
    "    for w, f in freq.items():\n",
    "        vocab[tuple(split_word(w))] += f\n",
    "    return vocab\n",
    "\n",
    "def get_pair_counts(vocab):\n",
    "    pairs = Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        for i in range(len(word)-1):\n",
    "            pairs[(word[i], word[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(vocab, pair):\n",
    "    a, b = pair\n",
    "    merged = a + b\n",
    "    new_vocab = Counter()\n",
    "\n",
    "    for word, freq in vocab.items():\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word)-1 and word[i] == a and word[i+1] == b:\n",
    "                new_word.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_vocab[tuple(new_word)] += freq\n",
    "\n",
    "    return new_vocab, merged\n",
    "\n",
    "def train_bpe(words, merge_steps=1000, vocab_size=None):\n",
    "    vocab = build_vocab(words)\n",
    "    merges = []\n",
    "    token_set = set([s for w in vocab for s in w])\n",
    "\n",
    "    for step in range(merge_steps):\n",
    "        pairs = get_pair_counts(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "\n",
    "        best = pairs.most_common(1)[0][0]\n",
    "        vocab, merged_sym = merge_vocab(vocab, best)\n",
    "        merges.append((best, merged_sym))\n",
    "        token_set.add(merged_sym)\n",
    "\n",
    "        if vocab_size and len(token_set) >= vocab_size:\n",
    "            break\n",
    "\n",
    "    final_vocab = set(token_set)\n",
    "    return merges, final_vocab\n",
    "\n",
    "def apply_bpe(word, merges):\n",
    "    tokens = split_word(word)\n",
    "\n",
    "    for pair, merged in merges:\n",
    "        i = 0\n",
    "        while i < len(tokens)-1:\n",
    "            if tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                tokens[i:i+2] = [merged]\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    if tokens[-1] == \"</w>\":\n",
    "        tokens = tokens[:-1]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38e418d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------- WORDPIECE IMPLEMENTATION -------------------------\n",
    "def train_wordpiece(words, vocab_size=32000, merge_steps=1000):\n",
    "    vocab = build_vocab(words)\n",
    "    merges = []\n",
    "    token_set = set([s for w in vocab for s in w])\n",
    "\n",
    "    for step in range(merge_steps):\n",
    "        pairs = get_pair_counts(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        best = pairs.most_common(1)[0][0]\n",
    "        merged = best[0] + best[1]   # simple merge\n",
    "        merges.append((best, merged))\n",
    "\n",
    "        new_vocab = Counter()\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if i < len(word)-1 and word[i] == best[0] and word[i+1] == best[1]:\n",
    "                    new_word.append(merged)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_vocab[tuple(new_word)] += freq\n",
    "        vocab = new_vocab\n",
    "        token_set.add(merged)\n",
    "\n",
    "        if len(token_set) >= vocab_size:\n",
    "            break\n",
    "\n",
    "    final_vocab = set(token_set)\n",
    "    return merges, final_vocab\n",
    "\n",
    "def apply_wordpiece(word, merges):\n",
    "    tokens = split_word(word)\n",
    "\n",
    "    for pair, merged in merges:\n",
    "        i = 0\n",
    "        while i < len(tokens)-1:\n",
    "            if tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                tokens[i:i+2] = [merged]\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    if tokens[-1] == \"</w>\":\n",
    "        tokens = tokens[:-1]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a75b7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokens: 25\n",
      "BPE vocab size: 129\n",
      "WordPiece vocab size: 129\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------- RUN TRAINING -------------------------\n",
    "words = load_corpus(\"corpus.txt\")\n",
    "print(\"Loaded tokens:\", len(words))\n",
    "\n",
    "# For your assignment:\n",
    "MERGES = 32000\n",
    "VOCAB = 32000\n",
    "\n",
    "bpe_merges, bpe_vocab = train_bpe(words, merge_steps=MERGES, vocab_size=VOCAB)\n",
    "wp_merges, wp_vocab = train_wordpiece(words, vocab_size=VOCAB, merge_steps=MERGES)\n",
    "\n",
    "print(\"BPE vocab size:\", len(bpe_vocab))\n",
    "print(\"WordPiece vocab size:\", len(wp_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826808f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:  byte pair encoding and wordpiece tokenization\n",
      "BPE → [' ', 'byt', 'e', ' ', 'pair', ' ', 'encod', 'ing', ' ', 'an', 'd', ' ', 'wordpiec', 'e', ' ', 'tokenization</w>']\n",
      "WordPiece → [' ', 'byt', 'e', ' ', 'pair', ' ', 'encod', 'ing', ' ', 'an', 'd', ' ', 'wordpiec', 'e', ' ', 'tokenization</w>']\n"
     ]
    }
   ],
   "source": [
    "sample =\" byte pair encoding and wordpiece tokenization\"\n",
    "print(\"Sample:\", sample)\n",
    "\n",
    "print(\"BPE →\", apply_bpe(sample, bpe_merges))\n",
    "print(\"WordPiece →\", apply_wordpiece(sample, wp_merges))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
