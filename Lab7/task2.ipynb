{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb4814b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 998000, Val size: 1000, Test size: 1000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "\n",
    "# --------------------------\n",
    "# 1Ô∏è‚É£ Load Sentences\n",
    "# --------------------------\n",
    "train_df = pd.read_csv(\"C:/Users/ashis/OneDrive/Desktop/NLP/Lab5/train_sentences.csv\")\n",
    "val_df   = pd.read_csv(\"C:/Users/ashis/OneDrive/Desktop/NLP/Lab5/val_sentences.csv\")\n",
    "test_df  = pd.read_csv(\"C:/Users/ashis/OneDrive/Desktop/NLP/Lab5/test_sentences.csv\")\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d80d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"sentence\"].isnull().sum())\n",
    "print(val_df[\"sentence\"].isnull().sum())\n",
    "print(test_df[\"sentence\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc9e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"sentence\"] = train_df[\"sentence\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fa6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------\n",
    "# 3Ô∏è‚É£ Build TF-IDF from scratch\n",
    "# --------------------------\n",
    "\n",
    "# Step 1: Build vocabulary from training data\n",
    "print(\"üîÑ Building vocabulary...\")\n",
    "vocab = set()\n",
    "for sentence in train_df[\"sentence\"]:\n",
    "    tokens = str(sentence).lower().split()\n",
    "    vocab.update(tokens)\n",
    "\n",
    "vocab = sorted(list(vocab))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Step 2: Calculate IDF (Inverse Document Frequency)\n",
    "print(\"üîÑ Calculating IDF...\")\n",
    "N = len(train_df)  # Total documents\n",
    "idf = {}\n",
    "\n",
    "for word in vocab:\n",
    "    doc_count = 0\n",
    "    for sentence in train_df[\"sentence\"]:\n",
    "        if word in str(sentence).lower().split():\n",
    "            doc_count += 1\n",
    "    idf[word] = math.log((N + 1) / (doc_count + 1)) + 1  # Adding 1 to avoid division by zero\n",
    "\n",
    "# Step 3: Calculate TF-IDF for each document\n",
    "def calculate_tfidf(sentences, idf, vocab, word_to_idx):\n",
    "    \"\"\"Calculate TF-IDF vectors for sentences\"\"\"\n",
    "    tfidf_matrix = np.zeros((len(sentences), len(vocab)))\n",
    "    \n",
    "    for doc_idx, sentence in tqdm(enumerate(sentences), total=len(sentences), desc=\"Computing TF-IDF\"):\n",
    "        tokens = str(sentence).lower().split()\n",
    "        \n",
    "        # Calculate term frequency\n",
    "        tf = Counter(tokens)\n",
    "        \n",
    "        # Calculate TF-IDF\n",
    "        for word, count in tf.items():\n",
    "            if word in word_to_idx:\n",
    "                word_idx = word_to_idx[word]\n",
    "                tf_value = count / len(tokens) if len(tokens) > 0 else 0\n",
    "                tfidf_matrix[doc_idx, word_idx] = tf_value * idf[word]\n",
    "    \n",
    "    return tfidf_matrix\n",
    "\n",
    "# Calculate TF-IDF for train, val, test\n",
    "print(\"üìä Calculating TF-IDF for train set...\")\n",
    "X_train = calculate_tfidf(train_df[\"sentence\"], idf, vocab, word_to_idx)\n",
    "\n",
    "print(\"üìä Calculating TF-IDF for validation set...\")\n",
    "X_val = calculate_tfidf(val_df[\"sentence\"], idf, vocab, word_to_idx)\n",
    "\n",
    "print(\"üìä Calculating TF-IDF for test set...\")\n",
    "X_test = calculate_tfidf(test_df[\"sentence\"], idf, vocab, word_to_idx)\n",
    "\n",
    "print(f\"\\nTrain TF-IDF shape: {X_train.shape}\")\n",
    "print(f\"Val TF-IDF shape: {X_val.shape}\")\n",
    "print(f\"Test TF-IDF shape: {X_test.shape}\")\n",
    "\n",
    "# --------------------------\n",
    "# 4Ô∏è‚É£ Save TF-IDF vectors to CSV\n",
    "# --------------------------\n",
    "print(\"\\nüíæ Saving TF-IDF to CSV files...\")\n",
    "\n",
    "def save_tfidf_to_csv(X, sentences, vocab, filename):\n",
    "    df_tfidf = pd.DataFrame(X, columns=vocab)\n",
    "    df_tfidf.insert(0, \"sentence\", sentences.values)\n",
    "    df_tfidf.to_csv(filename, index=False)\n",
    "    print(f\"  ‚úÖ Saved {filename}\")\n",
    "\n",
    "save_tfidf_to_csv(X_train, train_df[\"sentence\"], vocab, \"train_tfidf.csv\")\n",
    "save_tfidf_to_csv(X_val, val_df[\"sentence\"], vocab, \"val_tfidf.csv\")\n",
    "save_tfidf_to_csv(X_test, test_df[\"sentence\"], vocab, \"test_tfidf.csv\")\n",
    "\n",
    "# --------------------------\n",
    "# 5Ô∏è‚É£ Save TF-IDF matrices as NPZ (sparse format)\n",
    "# --------------------------\n",
    "print(\"\\nüíæ Saving TF-IDF matrices as NPZ...\")\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X_train_sparse = csr_matrix(X_train)\n",
    "X_val_sparse = csr_matrix(X_val)\n",
    "X_test_sparse = csr_matrix(X_test)\n",
    "\n",
    "scipy.sparse.save_npz(\"X_train_tfidf.npz\", X_train_sparse)\n",
    "scipy.sparse.save_npz(\"X_val_tfidf.npz\", X_val_sparse)\n",
    "scipy.sparse.save_npz(\"X_test_tfidf.npz\", X_test_sparse)\n",
    "\n",
    "print(\"‚úÖ TF-IDF from scratch computation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd89681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Nearest Neighbor search (within validation and test sets)\n",
    "# Uses cosine similarity on sparse TF-IDF matrices (closest other sentence in same set)\n",
    "# Outputs CSVs with: sentence, neighbor_sentence, neighbor_index, similarity\n",
    "# --------------------------\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure sparse matrices exist (created earlier)\n",
    "try:\n",
    "    X_val_sparse\n",
    "    X_test_sparse\n",
    "except NameError:\n",
    "    # Try loading from saved npz\n",
    "    import scipy.sparse\n",
    "    X_val_sparse = scipy.sparse.load_npz(\"X_val_tfidf.npz\")\n",
    "    X_test_sparse = scipy.sparse.load_npz(\"X_test_tfidf.npz\")\n",
    "\n",
    "# Helper to compute nearest neighbor within the same set (exclude self)\n",
    "def compute_self_nearest(X_sparse, sentences, out_csv):\n",
    "    n_samples = X_sparse.shape[0]\n",
    "    if n_samples <= 1:\n",
    "        print(f\"Not enough samples ({n_samples}) to compute neighbors for {out_csv}\")\n",
    "        return None\n",
    "\n",
    "    # NearestNeighbors with cosine returns distance = 1 - cosine_similarity\n",
    "    nn = NearestNeighbors(n_neighbors=2, metric='cosine', n_jobs=-1)\n",
    "    nn.fit(X_sparse)\n",
    "\n",
    "    distances, indices = nn.kneighbors(X_sparse, return_distance=True)\n",
    "    # distances[:,0] should be 0 (self), indices[:,0] == row index\n",
    "    # take second column as nearest neighbor excluding self\n",
    "    neigh_idx = indices[:, 1]\n",
    "    neigh_dist = distances[:, 1]\n",
    "    neigh_sim = 1.0 - neigh_dist  # convert to similarity\n",
    "\n",
    "    # Build DataFrame\n",
    "    rows = []\n",
    "    for i in range(n_samples):\n",
    "        sent = sentences.iloc[i] if hasattr(sentences, 'iloc') else sentences[i]\n",
    "        ni = int(neigh_idx[i])\n",
    "        neighbor_sent = sentences.iloc[ni] if hasattr(sentences, 'iloc') else sentences[ni]\n",
    "        sim = float(neigh_sim[i])\n",
    "        rows.append({\n",
    "            'sentence_index': i,\n",
    "            'sentence': sent,\n",
    "            'neighbor_index': ni,\n",
    "            'neighbor_sentence': neighbor_sent,\n",
    "            'similarity': sim\n",
    "        })\n",
    "\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    df_out.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved nearest-neighbor results ‚Üí {out_csv} ({len(df_out)} rows)\")\n",
    "    return df_out\n",
    "\n",
    "# Run for validation set\n",
    "print(\"Computing nearest neighbors for validation set...\")\n",
    "df_val_nn = compute_self_nearest(X_val_sparse, val_df['sentence'], \"val_nearest_neighbors.csv\")\n",
    "\n",
    "# Run for test set\n",
    "print(\"Computing nearest neighbors for test set...\")\n",
    "df_test_nn = compute_self_nearest(X_test_sparse, test_df['sentence'], \"test_nearest_neighbors.csv\")\n",
    "\n",
    "# Show top sample rows\n",
    "if df_val_nn is not None:\n",
    "    print(\"\\nValidation sample matches:\")\n",
    "    display(df_val_nn.head())\n",
    "if df_test_nn is not None:\n",
    "    print(\"\\nTest sample matches:\")\n",
    "    display(df_test_nn.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
