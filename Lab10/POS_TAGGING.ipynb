{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46448a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HMM POS Tagger without classes — Viterbi + K‑fold CV\n",
    "Each token is of the form word_tag.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict, Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e18ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Parsing ----------------\n",
    "def parse_tagged_sentence(line):\n",
    "    tokens = line.strip().split()\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        m = re.search(r\"([_/])([^_/]+)$\", tok)\n",
    "        if m:\n",
    "            tag = m.group(2)\n",
    "            word = tok[:-(len(tag)+1)]\n",
    "        else:\n",
    "            if '-' in tok:\n",
    "                word, tag = tok.rsplit('-', 1)\n",
    "            else:\n",
    "                continue\n",
    "        out.append((word, tag))\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_corpus(path):\n",
    "    sents = []\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                sents.append(parse_tagged_sentence(line))\n",
    "    return sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daeaf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ---------------- Training ----------------\n",
    "def train_hmm(train_sents, emission_alpha=1.0, transition_alpha=1.0):\n",
    "    tag_counts = Counter()\n",
    "    emission = defaultdict(Counter)\n",
    "    transition = defaultdict(Counter)\n",
    "    vocab = set()\n",
    "    tags = set()\n",
    "\n",
    "    for sent in train_sents:\n",
    "        prev = '<s>'\n",
    "        tag_counts[prev] += 1\n",
    "        for w,t in sent:\n",
    "            vocab.add(w)\n",
    "            tags.add(t)\n",
    "            tag_counts[t] += 1\n",
    "            emission[t][w] += 1\n",
    "            transition[prev][t] += 1\n",
    "            prev = t\n",
    "        transition[prev]['</s>'] += 1\n",
    "        tag_counts['</s>'] += 1\n",
    "\n",
    "    return list(sorted(tags)), vocab, tag_counts, emission, transition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87389329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Probabilities ----------------\n",
    "def emission_logprob(tag, word, tag_counts, emission, vocab, alpha):\n",
    "    V = len(vocab)\n",
    "    num = emission[tag].get(word, 0) + alpha\n",
    "    den = tag_counts[tag] + alpha * (V + 1)\n",
    "    return math.log(num) - math.log(den)\n",
    "\n",
    "\n",
    "def transition_logprob(prev, nxt, tag_counts, transition, tags, alpha):\n",
    "    N = len(tags) + 2\n",
    "    num = transition[prev].get(nxt, 0) + alpha\n",
    "    den = tag_counts[prev] + alpha * N\n",
    "    return math.log(num) - math.log(den)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b09cdee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Viterbi ----------------\n",
    "def viterbi_decode(words, tags, tag_counts, emission, transition, vocab, ea, ta):\n",
    "    V = [{}]\n",
    "    B = [{}]\n",
    "\n",
    "    for tag in tags:\n",
    "        tp = transition_logprob('<s>', tag, tag_counts, transition, tags, ta)\n",
    "        ep = emission_logprob(tag, words[0], tag_counts, emission, vocab, ea)\n",
    "        V[0][tag] = tp + ep\n",
    "        B[0][tag] = '<s>'\n",
    "\n",
    "    for t in range(1, len(words)):\n",
    "        V.append({})\n",
    "        B.append({})\n",
    "        for curr in tags:\n",
    "            ep = emission_logprob(curr, words[t], tag_counts, emission, vocab, ea)\n",
    "            best_score, best_prev = None, None\n",
    "            for prev in tags:\n",
    "                score = V[t-1][prev] + transition_logprob(prev, curr, tag_counts, transition, tags, ta) + ep\n",
    "                if best_score is None or score > best_score:\n",
    "                    best_score, best_prev = score, prev\n",
    "            V[t][curr] = best_score\n",
    "            B[t][curr] = best_prev\n",
    "\n",
    "    best_score, last = None, None\n",
    "    for tag in tags:\n",
    "        score = V[-1][tag] + transition_logprob(tag, '</s>', tag_counts, transition, tags, ta)\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score, last = score, tag\n",
    "\n",
    "    seq = [last]\n",
    "    for t in range(len(words)-1, 0, -1):\n",
    "        seq.append(B[t][seq[-1]])\n",
    "    return list(reversed(seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d55147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Evaluation ----------------\n",
    "def evaluate(true_sents, pred_sents):\n",
    "    tp = Counter(); fp = Counter(); fn = Counter();\n",
    "    tags = set()\n",
    "    for s in true_sents:\n",
    "        for _,t in s: tags.add(t)\n",
    "    for s in pred_sents:\n",
    "        for t in s: tags.add(t)\n",
    "\n",
    "    for gold, pred in zip(true_sents, pred_sents):\n",
    "        gtags = [t for _,t in gold]\n",
    "        for g,p in zip(gtags, pred):\n",
    "            if g == p: tp[g]+=1\n",
    "            else: fp[p]+=1; fn[g]+=1\n",
    "\n",
    "    per = {}\n",
    "    for tag in sorted(tags):\n",
    "        P = tp[tag]/(tp[tag]+fp[tag]) if tp[tag]+fp[tag]>0 else 0\n",
    "        R = tp[tag]/(tp[tag]+fn[tag]) if tp[tag]+fn[tag]>0 else 0\n",
    "        F = 2*P*R/(P+R) if P+R>0 else 0\n",
    "        per[tag] = (P,R,F)\n",
    "\n",
    "    TP = sum(tp.values()); FP = sum(fp.values()); FN = sum(fn.values())\n",
    "    microP = TP/(TP+FP) if TP+FP>0 else 0\n",
    "    microR = TP/(TP+FN) if TP+FN>0 else 0\n",
    "    microF = 2*microP*microR/(microP+microR) if microP+microR>0 else 0\n",
    "\n",
    "    macroF = sum(f for _,_,f in per.values()) / len(per)\n",
    "\n",
    "    return per, (microP,microR,microF), macroF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3592cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- K-Fold ----------------\n",
    "def kfold(data, K, seed=42):\n",
    "    random.Random(seed).shuffle(data)\n",
    "    n = len(data)\n",
    "    return [data[i*n//K:(i+1)*n//K] for i in range(K)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df71c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: microF=0.8562, macroF=0.6508\n",
      "Fold 2: microF=0.8612, macroF=0.6587\n",
      "Fold 3: microF=0.8534, macroF=0.6799\n",
      "Fold 4: microF=0.8618, macroF=0.6661\n",
      "Fold 5: microF=0.8560, macroF=0.6586\n",
      "Overall microF: 0.8577030071690608\n",
      "Overall macroF: 0.662802891848155\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- Main runner ----------------\n",
    "def run(path, K=5, ea=1.0, ta=1.0):\n",
    "    data = load_corpus(path)\n",
    "    folds = kfold(data, K)\n",
    "    results = []\n",
    "\n",
    "    for i in range(K):\n",
    "        test = folds[i]\n",
    "        train = [s for j,f in enumerate(folds) if j!=i for s in f]\n",
    "\n",
    "        tags, vocab, tag_counts, emission, transition = train_hmm(train, ea, ta)\n",
    "\n",
    "        preds = []\n",
    "        for sent in test:\n",
    "            words = [w for w,_ in sent]\n",
    "            pred = viterbi_decode(words, tags, tag_counts, emission, transition, vocab, ea, ta)\n",
    "            preds.append(pred)\n",
    "\n",
    "        per, micro, macroF = evaluate(test, preds)\n",
    "        results.append((micro, macroF))\n",
    "        print(f\"Fold {i+1}: microF={micro[2]:.4f}, macroF={macroF:.4f}\")\n",
    "\n",
    "    overall_microF = sum(m[0][2] for m in results)/K\n",
    "    overall_macroF = sum(m[1] for m in results)/K\n",
    "    print(\"Overall microF:\", overall_microF)\n",
    "    print(\"Overall macroF:\", overall_macroF)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run('wsj_pos_tagged_en.txt', K=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c908cb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence to tag (or press Enter to exit):\n",
      "Tagged output:\n",
      "Hi/NNP\n",
      ",/,\n",
      "how/WDT\n",
      "are/VBP\n",
      "you/.\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter a sentence to tag (or press Enter to exit):\")\n",
    "# user_sentence = input().strip()\n",
    "\n",
    "user_sentence='Hi , how are you'\n",
    "if user_sentence:\n",
    "# Load corpus again for training\n",
    "    data = load_corpus('wsj_pos_tagged_en.txt')\n",
    "    folds = kfold(data, 3)\n",
    "    train = [s for f in folds[1:] for s in f]\n",
    "    tags, vocab, tag_counts, emission, transition = train_hmm(train)\n",
    "    words = user_sentence.split()\n",
    "    pred = viterbi_decode(words, tags, tag_counts, emission, transition, vocab, 1.0, 1.0)\n",
    "    print(\"Tagged output:\")\n",
    "    for w,t in zip(words, pred):\n",
    "        print(f\"{w}/{t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc436328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
